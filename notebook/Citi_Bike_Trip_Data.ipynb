{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f923c35-bb4b-403b-9d41-ff88348ac802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11576\\1451669192.py:30: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV saved to: C:\\Users\\DELL\\Downloads\\combined_citibike_2023.csv\n"
     ]
    }
   ],
   "source": [
    "#fetch raw data\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "# Define your paths\n",
    "main_zip = r\"C:\\Users\\DELL\\Downloads\\2023-citibike-tripdata.zip\"\n",
    "monthly_zip_dir = r\"C:\\Users\\DELL\\Downloads\\2023-citibike-tripdata_temp_monthly_zips\"\n",
    "extract_folder = r\"C:\\Users\\DELL\\Downloads\\unzipped-2023-citibike-tripdata\"\n",
    "merged_csv_path = r\"C:\\Users\\DELL\\Downloads\\combined_citibike_2023.csv\"\n",
    "\n",
    "# Step 1: Unzip the main archive\n",
    "with zipfile.ZipFile(main_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(monthly_zip_dir)\n",
    "\n",
    "# Step 2: Extract CSVs from monthly zip files inside the extracted folder\n",
    "inner_dir = os.path.join(monthly_zip_dir, \"2023-citibike-tripdata\")\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(inner_dir):\n",
    "    if filename.endswith(\".zip\"):\n",
    "        path = os.path.join(inner_dir, filename)\n",
    "        with zipfile.ZipFile(path, 'r') as monthly_zip:\n",
    "            monthly_zip.extractall(extract_folder)\n",
    "\n",
    "# Step 3: Merge all CSVs into a single DataFrame\n",
    "df_list = []\n",
    "for f in os.listdir(extract_folder):\n",
    "    if f.endswith(\".csv\"):\n",
    "        file_path = os.path.join(extract_folder, f)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "combined_df.to_csv(merged_csv_path, index=False)\n",
    "\n",
    "print(f\"Combined CSV saved to: {merged_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "931a2bc2-b311-4e1c-8e00-ffe92c9ad36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Locations: ['W 21 St & 6 Ave', 'Broadway & W 58 St', 'West St & Chambers St']\n",
      "Filtered CSV saved at: C:\\Users\\DELL\\Downloads\\filtered_top3_citibike_2023.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged_csv_path = r\"C:\\Users\\DELL\\Downloads\\combined_citibike_2023.csv\"\n",
    "filtered_csv_path = r\"C:\\Users\\DELL\\Downloads\\filtered_top3_citibike_2023.csv\"\n",
    "\n",
    "chunk_size = 100_000  # adjust if needed\n",
    "top3_counter = {}\n",
    "\n",
    "# First pass: Count start station names across all chunks\n",
    "for chunk in pd.read_csv(merged_csv_path, chunksize=chunk_size, low_memory=False):\n",
    "    top_counts = chunk['start_station_name'].value_counts()\n",
    "    for station, count in top_counts.items():\n",
    "        top3_counter[station] = top3_counter.get(station, 0) + count\n",
    "\n",
    "# Get top 3 station names\n",
    "top3_stations = sorted(top3_counter, key=top3_counter.get, reverse=True)[:3]\n",
    "print(\"Top 3 Locations:\", top3_stations)\n",
    "\n",
    "# Second pass: Filter only top 3 stations and save to new file\n",
    "filtered_chunks = []\n",
    "for chunk in pd.read_csv(merged_csv_path, chunksize=chunk_size, low_memory=False):\n",
    "    filtered = chunk[chunk['start_station_name'].isin(top3_stations)]\n",
    "    filtered_chunks.append(filtered)\n",
    "\n",
    "# Concatenate filtered data and save\n",
    "pd.concat(filtered_chunks).to_csv(filtered_csv_path, index=False)\n",
    "print(f\"Filtered CSV saved at: {filtered_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a763f3d-98ac-4308-afb0-2d952f7d6ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved at: C:\\Users\\DELL\\Downloads\\cleaned_citibike_top3_2023.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtered_csv_path = r\"C:\\Users\\DELL\\Downloads\\filtered_top3_citibike_2023.csv\"\n",
    "cleaned_csv_path = r\"C:\\Users\\DELL\\Downloads\\cleaned_citibike_top3_2023.csv\"\n",
    "\n",
    "# Load filtered data\n",
    "df = pd.read_csv(filtered_csv_path)\n",
    "\n",
    "# Convert 'started_at' to datetime\n",
    "df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "df = df.dropna(subset=['started_at'])\n",
    "\n",
    "# Feature engineering\n",
    "df['date'] = df['started_at'].dt.date\n",
    "df['hour'] = df['started_at'].dt.hour\n",
    "df['weekday'] = df['started_at'].dt.day_name()\n",
    "\n",
    "# Save cleaned and enriched data\n",
    "df.to_csv(cleaned_csv_path, index=False)\n",
    "print(f\"Cleaned CSV saved at: {cleaned_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6987ffa-c9d8-4ced-8d71-a8650a347af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 14:52:22,045 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-05-09 14:52:22,050 INFO: Initializing external client\n",
      "2025-05-09 14:52:22,051 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 4.1.8 may not be compatible with the connected Hopsworks backend version 4.2.0. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 14:52:22,722 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1225931\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1225931/fs/1213514/fg/1440646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |███████████████████████| Rows 366490/366490 | Elapsed Time: 02:20 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: citibike_top3_2023_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1225931/jobs/named/citibike_top3_2023_1_offline_fg_materialization/executions\n",
      "Data uploaded successfully to Hopsworks!\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = r\"C:\\Users\\DELL\\Downloads\\cleaned_citibike_top3_2023.csv\"\n",
    "df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "# Convert object columns to string\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == \"object\":\n",
    "        df[col] = df[col].fillna(\"\").astype(str)\n",
    "\n",
    "#  Convert 'started_at' to datetime\n",
    "df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "\n",
    "# Check for rows with nulls after conversion\n",
    "df = df.dropna(subset=['started_at'])\n",
    "\n",
    "# Log in\n",
    "project = hopsworks.login(\n",
    "    project=\"Citi_Bike_TripData\",\n",
    "    api_key_value=\"eVrgcmUQIaYJz4kj.QNITpj9s3ieWAofZVNhhPtsjGng1ra5ZA9BsSGNRuI6i9WLGojdUuD0i0TBKfIx1\"\n",
    ")\n",
    "\n",
    "# Feature store connection\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "# Create the feature group\n",
    "fg = fs.create_feature_group(\n",
    "    name=\"citibike_top3_2023\",\n",
    "    version=1,\n",
    "    description=\"Top 3 Citi Bike stations from 2023, cleaned and enriched\",\n",
    "    primary_key=[\"start_station_name\", \"date\", \"hour\"],\n",
    "    event_time=\"started_at\"\n",
    ")\n",
    "\n",
    "# Insert the cleaned DataFrame\n",
    "fg.insert(df)\n",
    "\n",
    "print(\"Data uploaded successfully to Hopsworks!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6b6659b-379b-4506-b046-55316bb905ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 01:45:30,286 INFO: Initializing external client\n",
      "2025-05-10 01:45:30,286 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 4.1.8 may not be compatible with the connected Hopsworks backend version 4.2.0. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 01:45:34,434 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1225931\n",
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1225931/fs/1213514/fv/citibike_hourly_feature_view/version/1\n",
      "Feature view created successfully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "# Step 1: Login to Hopsworks\n",
    "project = hopsworks.login(\n",
    "    project=\"Citi_Bike_TripData\",\n",
    "    api_key_value=\"eVrgcmUQIaYJz4kj.QNITpj9s3ieWAofZVNhhPtsjGng1ra5ZA9BsSGNRuI6i9WLGojdUuD0i0TBKfIx1\"  # or use environment variable\n",
    ")\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "# Step 2: Get the existing feature group\n",
    "fg = fs.get_feature_group(name=\"citibike_features_hourly\", version=1)\n",
    "\n",
    "# Step 3: Select features and create a query\n",
    "query = fg.select_all()\n",
    "\n",
    "# Step 4: Create the feature view\n",
    "fv = fs.create_feature_view(\n",
    "    name=\"citibike_hourly_feature_view\",\n",
    "    version=1,\n",
    "    description=\"Feature view for Citi Bike demand prediction using hourly lag features\",\n",
    "    labels=[\"rides\"],\n",
    "    query=query\n",
    ")\n",
    "\n",
    "print(\"Feature view created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6104151-848f-4478-b4ea-861b9bcbe05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as SaiRishi9\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as SaiRishi9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"SaiRishi9/Citi_Bike_tripdata\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"SaiRishi9/Citi_Bike_tripdata\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository SaiRishi9/Citi_Bike_tripdata initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository SaiRishi9/Citi_Bike_tripdata initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='SaiRishi9', repo_name='Citi_Bike_tripdata', mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffdd6624-74d9-4698-acf7-aef263deb2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Baseline_Lag1 at: https://dagshub.com/SaiRishi9/Citi_Bike_tripdata.mlflow/#/experiments/0/runs/e2550eb01dcf46feb1549baad0376e4f\n",
      "🧪 View experiment at: https://dagshub.com/SaiRishi9/Citi_Bike_tripdata.mlflow/#/experiments/0\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000669 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 6985, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 16.132283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/09 21:40:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_All28Lags at: https://dagshub.com/SaiRishi9/Citi_Bike_tripdata.mlflow/#/experiments/0/runs/0dfd858853df4d9d9efa0f2a96c24edc\n",
      "🧪 View experiment at: https://dagshub.com/SaiRishi9/Citi_Bike_tripdata.mlflow/#/experiments/0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 800\n",
      "[LightGBM] [Info] Number of data points in the train set: 6985, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 16.132283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/09 21:40:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_Top10Lags at: https://dagshub.com/SaiRishi9/Citi_Bike_tripdata.mlflow/#/experiments/0/runs/ab1da64f1e304ee1b546796a64c23ba3\n",
      "🧪 View experiment at: https://dagshub.com/SaiRishi9/Citi_Bike_tripdata.mlflow/#/experiments/0\n",
      "\n",
      " Modeling complete. MAE summary:\n",
      "Baseline Lag-1: MAE = 5.743\n",
      "LightGBM - 28 Lags: MAE = 4.023\n",
      "LightGBM - Top 10 Lags: MAE = 4.183\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Load and clean dataset\n",
    "# ------------------------------\n",
    "df = pd.read_csv(\"C:/Users/DELL/Downloads/cleaned_citibike_top3_2023.csv\", low_memory=False)\n",
    "\n",
    "df[\"started_at\"] = pd.to_datetime(df[\"started_at\"])\n",
    "df = df.sort_values(\"started_at\")\n",
    "\n",
    "# Use only 1 station to simplify modeling\n",
    "top_station = df[\"start_station_name\"].value_counts().idxmax()\n",
    "df = df[df[\"start_station_name\"] == top_station]\n",
    "\n",
    "# Set rides as 1 per row, aggregate by hour\n",
    "df[\"rides\"] = 1\n",
    "df = df.groupby(\"started_at\").agg({\"rides\": \"sum\"}).reset_index()\n",
    "df.set_index(\"started_at\", inplace=True)\n",
    "df = df.resample(\"H\").sum().fillna(0)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# --------------------------------\n",
    "# Step 2: Create lag features\n",
    "# --------------------------------\n",
    "for lag in range(1, 29):\n",
    "    df[f\"lag_{lag}\"] = df[\"rides\"].shift(lag)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Split data\n",
    "# ------------------------------\n",
    "X = df[[f\"lag_{i}\" for i in range(1, 29)]]\n",
    "y = df[\"rides\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Start MLflow experiment\n",
    "# ------------------------------\n",
    "mlflow.set_experiment(\"citibike_modeling_experiment\")\n",
    "results = {}\n",
    "\n",
    "# 1. Baseline model (Naive lag-1)\n",
    "with mlflow.start_run(run_name=\"Baseline_Lag1\"):\n",
    "    y_pred = X_test[\"lag_1\"]\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mlflow.log_param(\"model\", \"Baseline\")\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    results[\"Baseline Lag-1\"] = mae\n",
    "\n",
    "# 2. LightGBM with all 28 lags\n",
    "with mlflow.start_run(run_name=\"LightGBM_All28Lags\"):\n",
    "    model = lgb.LGBMRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mlflow.log_param(\"model\", \"LightGBM-28-lags\")\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    results[\"LightGBM - 28 Lags\"] = mae\n",
    "\n",
    "# 3. LightGBM with Top 10 selected lags\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=10)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "selected_columns = [f\"lag_{i+1}\" for i in selected_indices]\n",
    "\n",
    "X_train_top = X_train[selected_columns]\n",
    "X_test_top = X_test[selected_columns]\n",
    "y_train_top = y_train\n",
    "y_test_top = y_test\n",
    "\n",
    "with mlflow.start_run(run_name=\"LightGBM_Top10Lags\"):\n",
    "    model = lgb.LGBMRegressor(random_state=42)\n",
    "    model.fit(X_train_top, y_train_top)\n",
    "    y_pred = model.predict(X_test_top)\n",
    "    mae = mean_absolute_error(y_test_top, y_pred)\n",
    "    mlflow.log_param(\"model\", \"LightGBM-Top10\")\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    results[\"LightGBM - Top 10 Lags\"] = mae\n",
    "\n",
    "# ------------------------------\n",
    "# Step 5: Print results\n",
    "# ------------------------------\n",
    "print(\"\\n Modeling complete. MAE summary:\")\n",
    "for model_name, mae_val in results.items():\n",
    "    print(f\"{model_name}: MAE = {mae_val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e10a542-9096-4faa-b768-f44d9a932c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdaf0495f62845fb8658fd52a6de028b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rides: [1.62019053]\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "model_uri = \"models:/citibike_demand_model/1\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Sample inference\n",
    "sample_input = X_test.iloc[[0]]  # replace with your own sample\n",
    "prediction = model.predict(sample_input)\n",
    "print(\"Predicted rides:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d00610c9-edf2-4ec0-bc65-d54b907ec4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:16:30,172 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-05-09 21:16:30,181 INFO: Initializing external client\n",
      "2025-05-09 21:16:30,181 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 4.1.8 may not be compatible with the connected Hopsworks backend version 4.2.0. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:16:30,835 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1225931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |███████████████████████████| Rows 5000/5000 | Elapsed Time: 00:04 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: citibike_features_hourly_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1225931/jobs/named/citibike_features_hourly_1_offline_fg_materialization/executions\n",
      "2025-05-09 21:16:47,662 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2025-05-09 21:16:50,759 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-05-09 21:18:23,435 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-05-09 21:18:23,513 INFO: Waiting for log aggregation to finish.\n",
      "2025-05-09 21:18:31,831 INFO: Execution finished successfully.\n",
      "Inserted rows 0 to 4999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |███████████████████████████| Rows 3817/3817 | Elapsed Time: 00:04 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: citibike_features_hourly_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1225931/jobs/named/citibike_features_hourly_1_offline_fg_materialization/executions\n",
      "2025-05-09 21:18:47,160 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2025-05-09 21:18:50,264 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-05-09 21:20:16,623 INFO: Waiting for execution to finish. Current state: SUCCEEDING. Final status: UNDEFINED\n",
      "2025-05-09 21:20:22,792 INFO: Waiting for execution to finish. Current state: FINISHED. Final status: SUCCEEDED\n",
      "2025-05-09 21:20:23,021 INFO: Waiting for log aggregation to finish.\n",
      "2025-05-09 21:20:23,023 INFO: Execution finished successfully.\n",
      "Inserted rows 5000 to 8816\n",
      "Feature group successfully inserted into Hopsworks.\n"
     ]
    }
   ],
   "source": [
    "# src/feature_pipeline.py\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "\n",
    "# Step 1: Load raw Citi Bike trip data\n",
    "csv_path = r\"C:\\Users\\DELL\\Downloads\\cleaned_citibike_top3_2023.csv\"\n",
    "df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "# Step 2: Convert started_at to datetime and aggregate to hourly ride counts\n",
    "df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "df['rides'] = 1  # each row is a ride\n",
    "df = df.set_index('started_at').resample('H').sum().reset_index()\n",
    "\n",
    "# Step 3: Create lag features\n",
    "for lag in range(1, 29):\n",
    "    df[f'lag_{lag}'] = df['rides'].shift(lag)\n",
    "\n",
    "# Step 4: Drop missing rows due to lagging\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 5: Extract additional columns for keys\n",
    "df['date'] = df['started_at'].dt.floor('D')  # datetime-compatible type\n",
    "df['hour'] = df['started_at'].dt.hour\n",
    "df['start_station_name'] = \"Top_Station\"  # replace with actual if available\n",
    "\n",
    "# Step 6: Keep only relevant columns\n",
    "features = [f'lag_{i}' for i in range(1, 29)] + ['date', 'hour', 'start_station_name', 'rides']\n",
    "df = df[features]\n",
    "\n",
    "# Step 7: Connect to Hopsworks\n",
    "project = hopsworks.login(\n",
    "    project=\"Citi_Bike_TripData\",\n",
    "    api_key_value=\"eVrgcmUQIaYJz4kj.QNITpj9s3ieWAofZVNhhPtsjGng1ra5ZA9BsSGNRuI6i9WLGojdUuD0i0TBKfIx1\"\n",
    ")\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "# Step 8: Create or get the feature group\n",
    "fg = fs.get_or_create_feature_group(\n",
    "    name=\"citibike_features_hourly\",\n",
    "    version=1,\n",
    "    primary_key=[\"start_station_name\", \"date\", \"hour\"],\n",
    "    event_time=\"date\",\n",
    "    description=\"Hourly lag features for Citi Bike demand prediction\"\n",
    ")\n",
    "\n",
    "# Step 9: Insert data in chunks to avoid disconnect errors\n",
    "batch_size = 5000\n",
    "for i in range(0, len(df), batch_size):\n",
    "    chunk = df.iloc[i:i+batch_size]\n",
    "    fg.insert(chunk, wait=True)\n",
    "    print(f\"Inserted rows {i} to {i+len(chunk)-1}\")\n",
    "\n",
    "print(\"Feature group successfully inserted into Hopsworks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81937db-61f3-4c2f-9df5-27c4305e1bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
